WEBVTT

1
00:00:08.309 --> 00:00:14.160
Michael Shannon: Alright, welcome back from break everybody and let's see here qualitative analysis.

2
00:00:15.330 --> 00:00:25.770
Michael Shannon: Historically, widely used, even by organizations like nist do D O was the list goes on still prevalent.

3
00:00:27.030 --> 00:00:30.270
Michael Shannon: where you have the elements of.

4
00:00:31.500 --> 00:00:46.080
Michael Shannon: risk and vulnerability right, we have the impactor magnitude, we have that across the horizontal on a scale of one to five, we have likelihood probability.

5
00:00:47.010 --> 00:01:02.700
Michael Shannon: On the vertical on a scale of one to five, and then we apply this usually over a certain timeframe, to a certain asset or an asset class in a particular scenario.

6
00:01:06.570 --> 00:01:20.340
Michael Shannon: And that would be populating your risk register or your risk ledger possibly massive amounts of spreadsheets may be tied to database processes and other analytics.

7
00:01:21.780 --> 00:01:38.010
Michael Shannon: OK, so the strength of qualitative is that it's been around for a long time, it is valuable, especially in a mature organization that has lots of use cases and experience with different types of events and incidents.

8
00:01:39.510 --> 00:01:41.700
Michael Shannon: A high level of expert judgment.

9
00:01:42.750 --> 00:01:48.120
Michael Shannon: subject matter, expertise and again like I said.

10
00:01:49.500 --> 00:01:52.110
Michael Shannon: Often tightly coupled with.

11
00:01:53.280 --> 00:02:05.220
Michael Shannon: Insurance companies and other types of entities that can provide a lot of data for them, the more data, you have the more real experiences you've had the more accurate, this is going to be.

12
00:02:06.360 --> 00:02:07.200
Michael Shannon: Qualitative.

13
00:02:08.820 --> 00:02:25.020
Michael Shannon: What are some of the downsides if I use simply qualitative analysis to go justify the fact that i'm going to accept risk or mitigate risk or whatever to a steering committee or a team.

14
00:02:26.790 --> 00:02:28.350
Michael Shannon: i'm not really delivering.

15
00:02:29.910 --> 00:02:35.490
Michael Shannon: Real numbers i'm delivering a heat map that says okay.

16
00:02:36.570 --> 00:02:53.970
Michael Shannon: A certain event that can happen to a certain asset, if it is critical in nature and its magnitude and it happens likely or frequently that's going to put it into the high heat map, therefore, we need to spend.

17
00:02:55.140 --> 00:03:02.460
Michael Shannon: More money or our resources to protect that asset from that negative occurrence.

18
00:03:04.050 --> 00:03:29.310
Michael Shannon: The challenges with this, though, are you have a scale of one to five, or one to eight or one to 10 but there's there's no nuance here, it would be nice if I had 4.3 or 4.4 because something's either likely in However we define that or occasional However we define that.

19
00:03:31.020 --> 00:03:40.530
Michael Shannon: So it's it's it's more likely to have subjectivity bias and agenda behind it.

20
00:03:43.680 --> 00:03:48.090
Michael Shannon: there's no to there's no minor nothing between minor and moderate.

21
00:03:49.380 --> 00:03:58.050
Michael Shannon: So it would be helpful if there is some real values attached to this so there's a saying that we have.

22
00:03:58.560 --> 00:04:07.440
Michael Shannon: In the United States, I don't know the English language you don't throw out the baby with the bathwater right once you decide that the bathwater.

23
00:04:07.980 --> 00:04:20.760
Michael Shannon: No longer has a use or the use that it had you don't throw the baby out with the bathwater, well, we don't throw out our qualitative analysis, so what we would do probably maybe as a.

24
00:04:21.960 --> 00:04:37.740
Michael Shannon: workaround or as a transition to a fully quanta approach would be semi Croissant where you want to put some real values on those numbers so for your organization.

25
00:04:38.760 --> 00:04:46.530
Michael Shannon: Minor at number two means less than a million dollars now take these numbers.

26
00:04:47.670 --> 00:04:57.870
Michael Shannon: With a grain of salt it varies depending on the Organization for some organizations minor at number two could be less than 10 million.

27
00:04:59.130 --> 00:05:21.990
Michael Shannon: Okay, you get the idea so taking the same subject matter, experts and expert judgment that you have an experience your team, whether it's part of a workshop or a risk analysis, whatever methodology you're going to try to put real values on it and on likelihood seldom.

28
00:05:23.070 --> 00:05:30.390
Michael Shannon: hasn't happened in five years occasional it's happened it's happened once in the last five years, but not in the last year.

29
00:05:32.190 --> 00:05:40.650
Michael Shannon: So so we're not trying to memorize these distinct is we're trying to say with a semi Croissant i'm trying to put some real numbers on it.

30
00:05:41.040 --> 00:06:00.480
Michael Shannon: Some real currency figures and some real values to come up with something that is more than just something that is relative on a relative basis less subjective more objective and then now what we're providing the meaningful metrics that we're providing.

31
00:06:01.530 --> 00:06:07.800
Michael Shannon: To the decision makers have more meat they have more weight, because they have numbers behind them.

32
00:06:08.880 --> 00:06:10.440
Michael Shannon: And it's also helpful because.

33
00:06:11.580 --> 00:06:12.870
Michael Shannon: we're also determining.

34
00:06:14.040 --> 00:06:27.660
Michael Shannon: How what how much resources we're going to spend to protect assets right there's another old saying we have in Texas you don't put $100 fins around a $10 mule.

35
00:06:29.730 --> 00:06:44.790
Michael Shannon: So you know part of this is we're not going to spend $100,000 to protect an asset that has a value and the ability to generate revenue of $10,000 at least we hope not.

36
00:06:47.460 --> 00:06:57.600
Michael Shannon: Now, when it comes to quantitative analysis, the one that you probably learned first was this classic Whitman model.

37
00:06:58.260 --> 00:07:08.700
Michael Shannon: This is Dr Michael Whitman his classic model and, by the way, it does have the elements of risk or vulnerability in it, because the Ai le.

38
00:07:09.240 --> 00:07:23.250
Michael Shannon: Is the annualized last expectancy, what do I expect to lose on either a fiscal year annual year calendar year so there's that you've got the timeframe.

39
00:07:24.060 --> 00:07:34.770
Michael Shannon: you've got magnitude, because you have the X asset value and the last expectancy, you have the probability or likelihood that's in the exposure factor.

40
00:07:35.490 --> 00:07:44.910
Michael Shannon: So you have those elements, but they need to be more quantified Okay, and I could spend a half an hour on this, but i'm not going to do that.

41
00:07:46.350 --> 00:07:50.190
Michael Shannon: which you might want to do if you haven't gone through this yourself.

42
00:07:51.270 --> 00:08:03.270
Michael Shannon: there's several YouTube videos that will walk through difference here's one, but there are several YouTube videos that they don't cost you anything and they'll walk through the Whitman model.

43
00:08:03.840 --> 00:08:17.610
Michael Shannon: Like there'll be on a whiteboard and they'll use, you know what you know front end web servers or the database they'll they'll use different scenarios assets under a certain.

44
00:08:18.690 --> 00:08:27.300
Michael Shannon: You know, particular event or occurrence and walk through these for you, so you will need to be.

45
00:08:28.320 --> 00:08:49.530
Michael Shannon: familiar with this class of model plastic model which is going to deliver some meaningful metrics and real values right, you know i'm going to give you an annualized loss expectancy of $10,000 on these public facing sharepoint servers running in the dmz.

46
00:08:52.890 --> 00:09:02.310
Michael Shannon: A more popular and emerging method for quantitative and I, and I, I know this just not only from my own experience, and I have a certification in it.

47
00:09:03.480 --> 00:09:05.940
Michael Shannon: But uh we hear back from a lot of our.

48
00:09:07.830 --> 00:09:13.020
Michael Shannon: Candidates i'll call you not students, but our co our peers that are going through.

49
00:09:14.160 --> 00:09:27.300
Michael Shannon: certification processes and in the job market and i'll i'll let people tell me yeah when I when I went to interview they asked me if I was familiar with fair, open, fair.

50
00:09:29.130 --> 00:09:49.110
Michael Shannon: So one of the advantages of open, fair, is, I have the ability to not only decompose some of these values and break them down from the quants from the classic want I can get more granular by decomposing and i'll show you this in a second but you're also combining.

51
00:09:50.190 --> 00:10:11.820
Michael Shannon: Your analysis with tools like pert tools, yes you'll need to know that formula right, so let me go back to that formula so definitely know that the annualized loss expectancy is a product of this single loss expectancy and the ar oh mathematically.

52
00:10:15.210 --> 00:10:30.870
Michael Shannon: So with with fair we're going to be applying this risk to a particular asset or asset class under a particular scenario for a time frame, there are two elements are the last event frequency.

53
00:10:32.460 --> 00:10:32.970
Michael Shannon: Okay.

54
00:10:34.320 --> 00:10:51.180
Michael Shannon: Which is their their kind of their probability or likelihood the loss of in frequency and then their impact would be last magnitude and then they break down those values, so the last event frequency, they would consider.

55
00:10:52.560 --> 00:10:54.900
Michael Shannon: To be the.

56
00:10:56.040 --> 00:11:11.970
Michael Shannon: frequency of the event or the negative occurrence and vulnerability and, by the way, vulnerability is not a scale of one to 10, this is a mathematical value like Point seven eight or point nine.

57
00:11:13.260 --> 00:11:18.840
Michael Shannon: Okay now i'll just say real quickly from the standpoint of fair.

58
00:11:19.860 --> 00:11:34.530
Michael Shannon: What is their goal well their goal is to be as accurate as possible but technically their goal is like a dartboard or a roulette wheel, where you are in the 90 percentile.

59
00:11:36.060 --> 00:11:39.150
Michael Shannon: Okay, so if you're a spinning a roulette wheel.

60
00:11:40.200 --> 00:12:01.770
Michael Shannon: You know you got 10% of those numbers you don't want to land on or on a dartboard you've got 10% of the dartboard that you don't want you can't hit so they're shooting for 90% accuracy, they don't really use the word accuracy, because what fair uses is what they consider to be.

61
00:12:04.680 --> 00:12:21.120
Michael Shannon: Calculations okay so here's here's my point vulnerability, how do we get to some type of measurement you know, have a percentile of vulnerability they'll get they're going to break it down.

62
00:12:22.230 --> 00:12:31.380
Michael Shannon: they're going to decompose it okay so they're going to say vulnerabilities now broken down into what is the capability of the threat actor.

63
00:12:32.580 --> 00:12:47.640
Michael Shannon: And that's going to be, you know some number, what would be considered to be the capability of threat actor so, for example, if it's the cleaning crew who's coming into the office and the CEOs laptop.

64
00:12:49.230 --> 00:12:53.640
Michael Shannon: What is the threat capability of that licensed and bonded cleaning crew.

65
00:12:55.080 --> 00:12:58.890
Michael Shannon: Or the crew doing the pest control.

66
00:12:59.910 --> 00:13:05.520
Michael Shannon: Right can or compared to the threat capability of a security guard.

67
00:13:07.080 --> 00:13:14.220
Michael Shannon: I would say the threat capability of a security guard is probably going to be higher than the capability of the cleaning crew.

68
00:13:15.480 --> 00:13:33.270
Michael Shannon: And also vulnerability is determining difficulty, and that is a value of what type of controls, what is your inherent risk what controls are put in place right now, and so they use PR.

69
00:13:34.680 --> 00:13:49.710
Michael Shannon: Calculations they use money, Carlo simulations so they'll basically you know these are calibrated estimates and what what what they are so what you would do with your team that's doing fair is you would start out with your extremes.

70
00:13:50.970 --> 00:14:01.230
Michael Shannon: This the extreme and that's that's kind of your universe, then you kind of work, your way back in by removing and just brainstorming.

71
00:14:02.130 --> 00:14:15.420
Michael Shannon: And workshopping to you get a value that you feel confident that's in the 90 percentile then you'll run this through thousands upon thousands upon thousands of Monte Carlo simulations.

72
00:14:16.560 --> 00:14:19.320
Michael Shannon: And you'll come up with a value and that's what you present.

73
00:14:20.910 --> 00:14:24.390
Michael Shannon: that's what you present to your decision makers your CT.

74
00:14:26.070 --> 00:14:34.050
Michael Shannon: And the results should be better than just a simple qualitative heat map.

75
00:14:35.220 --> 00:14:57.210
Michael Shannon: don't forget the categories of controls OK, so the CIS SP uses the nist i'm sorry they use the traditional categories nist uses others, this is one of those areas where they deviate Okay, the nist control categories are.

76
00:14:58.260 --> 00:15:02.820
Michael Shannon: managerial operational and technical.

77
00:15:04.800 --> 00:15:12.630
Michael Shannon: These are the ones that we want to know policy is SP administrative or managerial technical and physical.

78
00:15:14.040 --> 00:15:16.980
Michael Shannon: And you know when it comes to questions on this.

79
00:15:18.150 --> 00:15:19.620
Michael Shannon: don't overthink it.

80
00:15:21.030 --> 00:15:28.050
Michael Shannon: there's overlap okay there's overlap, obviously, if I say intrusion detection.

81
00:15:29.250 --> 00:15:30.360
Michael Shannon: intrusion detection.

82
00:15:31.440 --> 00:15:39.930
Michael Shannon: Well intrusion detection as a control, well, it is a technical control because it could be an ids or an ips sensor but.

83
00:15:40.980 --> 00:15:42.180
Michael Shannon: You know, a.

84
00:15:43.380 --> 00:15:44.070
Michael Shannon: camera.

85
00:15:45.720 --> 00:15:57.690
Michael Shannon: or motion detector is an intrusion detection, even a guard is an intrusion detection so there's overlap, but we don't want to overthink it okay.

86
00:16:01.230 --> 00:16:09.570
Michael Shannon: These are the ones where I want to make sure that you don't get burned on this one, and again we don't overthink these either.

87
00:16:11.190 --> 00:16:16.500
Michael Shannon: But a lock a locking mechanism is a preventative.

88
00:16:17.730 --> 00:16:22.440
Michael Shannon: Control according to isc squared and others.

89
00:16:24.360 --> 00:16:42.360
Michael Shannon: We can sit here and argue that it's not really a preventative mechanism that a locking mechanism, technically speaking, is a determined, because all a lock really does is slow down the attacker because every lock can be overcome with brute force any lock.

90
00:16:44.220 --> 00:16:45.960
Michael Shannon: can be overcome with brute force.

91
00:16:48.480 --> 00:17:05.280
Michael Shannon: explosive a bazooka I don't care so technically yes any lock can be overcome with brute force, therefore, you could you could say it just really deters the attacker but don't overthink it, it is a preventative control.

92
00:17:06.420 --> 00:17:19.500
Michael Shannon: Now the effectiveness of the preventative control is that subject to discussion, but a locking mechanism is a preventative control and we usually combine other controls with it.

93
00:17:22.410 --> 00:17:24.840
Michael Shannon: So the one here, I want to talk about that may be.

94
00:17:26.160 --> 00:17:34.620
Michael Shannon: Confusing is compensating because it says it aids a control already in place that's one way to look at a compensating control.

95
00:17:35.970 --> 00:17:41.040
Michael Shannon: For example, you know we have integrity mechanisms.

96
00:17:42.570 --> 00:17:47.880
Michael Shannon: We have authentic authentication and authorization, however.

97
00:17:49.350 --> 00:18:00.150
Michael Shannon: We still have the ability to prevent certain highly privileged users for doing things that they may have permissions to do in the directory service.

98
00:18:02.010 --> 00:18:13.140
Michael Shannon: Because we're going to put a separation of duties or segregation of duties policy in place, so you have two or three administrators or two or three developers or engineers.

99
00:18:13.500 --> 00:18:23.940
Michael Shannon: who may have the permissions and rights to do something, but based on policy or based on what they're doing in their job role shouldn't be changing.

100
00:18:24.840 --> 00:18:34.710
Michael Shannon: So we have separation or segregation of duties that is a type of compensating control, let me give another example of a compensating control that may come into play on the exam.

101
00:18:35.970 --> 00:18:37.650
Michael Shannon: You have determined that.

102
00:18:39.180 --> 00:18:40.440
Michael Shannon: You work for a university.

103
00:18:41.460 --> 00:18:52.980
Michael Shannon: And you have a lot you've had lots of security breaches and security problems but you're if you're a small university or a small college, so you don't have a huge budget.

104
00:18:53.670 --> 00:19:04.860
Michael Shannon: So you go to the Steering Committee or the board and you say we really need to go and use a we don't have the people we don't have the personnel.

105
00:19:05.370 --> 00:19:17.820
Michael Shannon: So i'm suggesting that we go use a managed security service provider an MSP and I suggest we use for to coordinate and we use their for to gate products and others.

106
00:19:19.350 --> 00:19:31.140
Michael Shannon: Or, or we have a really big issue with endpoint protection and I recommend we go use Palo Alto networks new next generation endpoint protection.

107
00:19:31.830 --> 00:19:44.070
Michael Shannon: Okay, and then they come back to you and say your justification, we totally trust you and you're right on we just don't have the budget, right now, or we have to raise money.

108
00:19:44.700 --> 00:19:52.560
Michael Shannon: And then we can revisit this at the end of the Semester or the next quarter so that's what the committee tells you.

109
00:19:54.120 --> 00:20:08.190
Michael Shannon: Then they also say, but we do have this funding, we do have this ability to hire one more person, and you can do that, until we get the budget to go put in the four to net for the gates solution.

110
00:20:09.120 --> 00:20:20.370
Michael Shannon: So they give you some money they give you some resources and you go and you do some updates or upgrades or you add something or you hire a new person that's a compensating control.

111
00:20:21.960 --> 00:20:35.370
Michael Shannon: The compensating control in this case is a stopgap measure or a workaround until you can actually put in the optimal preventative detective corrective determine controls.

112
00:20:41.730 --> 00:20:56.400
Michael Shannon: All right, moving along this is the nist cybersecurity framework wouldn't hurt to review this the five areas identify you know what are your goals identify protect detect respond recover.

113
00:20:59.880 --> 00:21:20.460
Michael Shannon: If you're not familiar with the CIS Center for net security, this is this is playing into our risk analysis and risk management, this may be some guidance or best practices, you want to use not just best practices, but tools Threat Assessment awareness, they also have a top 20.

114
00:21:21.840 --> 00:21:22.710
Michael Shannon: controls.

115
00:21:23.730 --> 00:21:34.800
Michael Shannon: And a lot of organizations that are you know, on the Internet or the worldwide web will use their top 20 controls what's interesting kind of at the top of the list.

116
00:21:36.000 --> 00:21:41.850
Michael Shannon: Of the top 20 kind of goes back to where we started today hold on a second, I have to, I have to.

117
00:21:43.080 --> 00:21:47.580
Michael Shannon: I have to cough sorry so i'm gonna i'm gonna go off the camera for a second.

118
00:21:57.990 --> 00:21:58.860
Michael Shannon: That was violent.

119
00:22:00.780 --> 00:22:04.800
Michael Shannon: So they have a top 20 what's interesting we've got to the beginning of the day to day.

120
00:22:05.520 --> 00:22:21.600
Michael Shannon: Their top 20 basically says know what your hardware is and know what your software is that's where they kind of begin, you know it doesn't make any sense to start implementing any other controls until you know exactly what you have.

121
00:22:23.850 --> 00:22:44.160
Michael Shannon: goes without saying, right, but they also have benchmarks that are secure configurations, you know their images consensus created hardened database images applications, they have their controls, I mentioned the CIS controls, then they have a membership Program.

122
00:22:47.880 --> 00:22:54.390
Michael Shannon: Well, what happened was Mario I left my my I took my break and I went into the other room I left my water.

123
00:22:55.650 --> 00:22:59.130
Michael Shannon: But i'm not going to stop classical get my water i'll just tough it out.

124
00:23:01.410 --> 00:23:14.400
Michael Shannon: So cloud security alliance, this is a biggie if you're getting the cloud security professional I wouldn't say so much on this exam, but it is you know mentioned the objectives.

125
00:23:16.500 --> 00:23:25.620
Michael Shannon: Not just defining and raising awareness, but the CSA has a certification path for cloud auditing.

126
00:23:27.090 --> 00:23:31.410
Michael Shannon: They have a cloud control matrix and it's quite elaborate.

127
00:23:32.430 --> 00:23:52.800
Michael Shannon: version four so additionally what you would do if you're doing like a CSA assessment like you're assessing a software as a service provider right that you want to use you could give them first they have a questionnaire, so you got sorry I got it I go, I have to go get my water so.

128
00:23:53.820 --> 00:23:57.270
Michael Shannon: Give me about 30 seconds i'll be right back.

129
00:24:36.090 --> 00:24:38.640
Michael Shannon: sorry about that little allergies today.

130
00:24:42.150 --> 00:25:00.270
Michael Shannon: So the first step would be to use the kind of the consensus questionnaire and then you can go further in auditing and the cloud control matrix is like the auditing tool it's it's it's massive Okay, but that's one of the things they offer.

131
00:25:04.050 --> 00:25:07.170
Michael Shannon: So other frameworks Kobe five.

132
00:25:08.400 --> 00:25:17.280
Michael Shannon: pci DSS for bank cards and credit cards The is a CA has its risk it so you know, be aware of.

133
00:25:18.480 --> 00:25:19.020
Michael Shannon: You know.

134
00:25:20.430 --> 00:25:21.210
Michael Shannon: Those initiatives.

135
00:25:22.680 --> 00:25:31.290
Michael Shannon: Now, once you decide to implement controls and know from from the open, fair standpoint, you are raising difficulty.

136
00:25:31.980 --> 00:25:45.870
Michael Shannon: against the threat actors are threat agents you're raising your resistance level right and you're, the result will be a different residual risk that maps to your threat handling or your risk appetite right your.

137
00:25:47.940 --> 00:25:56.580
Michael Shannon: Risk treatment right so maybe a formal evaluation this formal evaluation by the way, may be done.

138
00:25:57.600 --> 00:26:05.730
Michael Shannon: This security control assessment may be part of a full blown security test this may be done in house.

139
00:26:06.150 --> 00:26:16.110
Michael Shannon: Or maybe going to be pulling in a third party and they'll do a wide variety of risk assessment vulnerability assessment penetration testing maybe hunt teams.

140
00:26:16.440 --> 00:26:30.240
Michael Shannon: Other types of things audits reviews okay just depends on how elaborate, but the goal often of this assessment of your controls is not just the gap analysis.

141
00:26:30.630 --> 00:26:41.520
Michael Shannon: to determine, you know where do we need to fill in our gaps to meet our risk treatment, but to me, maybe part of a more global type of understanding, your.

142
00:26:46.140 --> 00:26:59.310
Michael Shannon: Understanding your maturity level okay part of a capability maturity model now we're going to there's there's other maturity models, besides the cms, but this is the one that's really well known okay.

143
00:27:00.570 --> 00:27:09.360
Michael Shannon: When you get into more focused let's say we're just evaluating our maturity of our software development.

144
00:27:10.650 --> 00:27:21.450
Michael Shannon: Then, in that case you may use something a little bit more specific and focused like oh os has a software assurance maturity model called Sam.

145
00:27:22.710 --> 00:27:46.170
Michael Shannon: Now that may be used in conjunction with the aforementioned are part of the aforementioned security testing evaluation were you using the os Sam model to get you know you're assessing your software development, security you're determining how mature is your Dev SEC OPS.

146
00:27:48.450 --> 00:27:57.870
Michael Shannon: But this is more more of a global determine where you are okay Now I want you to think about this from a common sense standpoint and what I did was I put the.

147
00:27:58.800 --> 00:28:20.160
Michael Shannon: Traditional cms labels are initial repeatable defined managed and optimized the ones in parenthesis are the factor analysis of information risk labels, because they have their own approach to where they feel like they improved upon this model.

148
00:28:21.900 --> 00:28:23.970
Michael Shannon: And you know, obviously.

149
00:28:25.020 --> 00:28:28.860
Michael Shannon: You want to be as mature as possible but here's the takeaway.

150
00:28:31.110 --> 00:28:33.960
Michael Shannon: You, you must be at Level one.

151
00:28:35.310 --> 00:28:37.770
Michael Shannon: Either never.

152
00:28:38.880 --> 00:28:42.180
Michael Shannon: or for a shorter period as possible.

153
00:28:44.670 --> 00:28:46.500
Michael Shannon: So if you're a startup company.

154
00:28:47.910 --> 00:29:03.480
Michael Shannon: And you just you're just a bunch of developers they went to college together and you go least some space and we're going to do everything in the cloud, and you don't have anybody with security expertise, probably you're going to be at Level one.

155
00:29:04.890 --> 00:29:25.590
Michael Shannon: chaotic ad hoc if something happens, one of the partners will rise to the occasion and perform some heroics but it's a free for all you aren't using any type of best practices or guidance you're just relying totally on your cloud provider.

156
00:29:26.760 --> 00:29:36.540
Michael Shannon: So the first point is you really don't want to even start out at this level, and if you are, you want to get to level two as quickly as possible.

157
00:29:38.550 --> 00:29:56.640
Michael Shannon: Now is level to where you finally start to get some guidance and some best practices, where you have some decision making, it may be poor decision making, but you do have people in authority, you know you have a security administrator or whatever right.

158
00:29:58.740 --> 00:30:08.790
Michael Shannon: You have some roles and responsibilities right maybe you have a questionable risk assessment method, but you have some things in place.

159
00:30:09.420 --> 00:30:27.750
Michael Shannon: Is that still a level, we want to be at no, and this is a level, a lot of entities are at, especially small to medium sized organizations, you know the kind of business that has a network operating system and, like three people are sharing the admin password.

160
00:30:29.460 --> 00:30:36.240
Michael Shannon: And they have no multifactor authentication and the only physical security, I have yeah we have an alarm system.

161
00:30:38.490 --> 00:30:42.510
Michael Shannon: So we really would like to get to level three.

162
00:30:44.160 --> 00:30:52.500
Michael Shannon: That is the highest level that many organizations will ever hope to be at because of their budget and their resources and it's highly unlikely.

163
00:30:52.800 --> 00:31:05.280
Michael Shannon: That commercial organizations will ever get to this level that's a level that you would see like a military installation or a government intelligence agency or maybe a.

164
00:31:06.750 --> 00:31:10.110
Michael Shannon: A military contractor would get to.

165
00:31:11.550 --> 00:31:16.380
Michael Shannon: Purely explicit security in place the highest maturity level.

166
00:31:17.490 --> 00:31:27.120
Michael Shannon: Okay, but again, often determining where you are and where you want to be, is the result of security control assessment and testing.

167
00:31:27.990 --> 00:31:39.390
Michael Shannon: it's not it's obviously never enough from a security management standpoint to say okay i've gone to the decision makers they've given me the budget i've gone acquired.

168
00:31:39.810 --> 00:31:54.630
Michael Shannon: build or buy the solutions and put them in place they're obviously going to want accountability they're going to want to see what are the results of those investments in those various controls that you had to have.

169
00:31:59.040 --> 00:32:11.280
Michael Shannon: same systems are extremely popular systems now PR collecting and aggregating a wide variety of different sources of information collecting logs various logs.

170
00:32:12.180 --> 00:32:29.520
Michael Shannon: And there has to be some other processing done by the same system, usually because you have a lot of different systems if an event happens you'll have multiple systems sending logs alarms and alerts, for the same event.

171
00:32:31.320 --> 00:32:38.760
Michael Shannon: there's also you know different types of CES log right it depends on the operating system.

172
00:32:40.770 --> 00:32:51.210
Michael Shannon: You know, Microsoft has SIS logs and other types of third party logging tools, maybe that you have from you know solar winds.

173
00:32:54.030 --> 00:32:59.070
Michael Shannon: Linux has different SIS logs and audit D and other things like that.

174
00:32:59.880 --> 00:33:11.250
Michael Shannon: You have things sent from a wide variety of devices, so there needs to be de duplication some filtering basically the raw data needs to become information that can be analyzed.

175
00:33:11.610 --> 00:33:22.860
Michael Shannon: information that can generate reports some seems systems will do storage, others will pass that along to cloud storage or some something else in your data Center.

176
00:33:23.220 --> 00:33:36.120
Michael Shannon: Some of the same systems will present dashboards but often the same system will create a bunch a bunch of event classes and those will be what are sent to your source system.

177
00:33:38.760 --> 00:33:48.300
Michael Shannon: So they often work in combination, so you know obvious obvious wide variety of data sources and and, to some degree that's going to be automated.

178
00:33:49.050 --> 00:34:02.430
Michael Shannon: But then the orchestration part and the run books, with the playbooks that automate the response manual semi automated automated will be that's the responsibility of the source system.

179
00:34:04.170 --> 00:34:08.730
Michael Shannon: And when you mentioned something like I did earlier like Sentinel.

180
00:34:09.330 --> 00:34:23.790
Michael Shannon: that's a really good example of a cloud based theme and source system that can be used in a hybrid cloud environment, so if you if you are a windows environment and you're running active directory and you're using azure.

181
00:34:24.660 --> 00:34:33.120
Michael Shannon: As your provider that's a type of managed security service that you can use and have this cloud based semen source system.

182
00:34:35.160 --> 00:34:47.430
Michael Shannon: The main goal of sore is to automate and orchestrate threat and vulnerability management and response and other security operations that can be automated.

183
00:34:48.120 --> 00:35:01.770
Michael Shannon: And by doing that you're really removing a lot of the mundane and non critical tasks, so that security operators can be focused on one handling real incidence.

184
00:35:02.190 --> 00:35:15.240
Michael Shannon: And disaster recovery and continual improvement of the existing security controls the ongoing monitoring and visibility that they need to do that involves the manual process.

185
00:35:16.320 --> 00:35:36.630
Michael Shannon: Now the as the events are sent from the same system to the source system, the response can be defensive approach, a detective a responsive a remediation process need to install this or upgrade this and that and the more you can automate that the better.

186
00:35:40.620 --> 00:35:46.950
Michael Shannon: And anything that is routine or manual that can be repetitive repent repeated should be automated.

187
00:35:49.470 --> 00:35:51.000
Michael Shannon: reports that are generated.

188
00:35:52.080 --> 00:35:59.160
Michael Shannon: One reports not going to cut it Okay, so you need to think about the audience of the report you'll have a different report that you're going to.

189
00:35:59.760 --> 00:36:12.540
Michael Shannon: You know present to the C suite of the C team, then the then these reports you're going to present on Monday morning to your sock team your security operations Center.

190
00:36:13.620 --> 00:36:22.680
Michael Shannon: Or maybe you have a different report that you give to your CIO or your ci so, then you would give to let's say a chief technology officer.

191
00:36:23.940 --> 00:36:27.330
Michael Shannon: who understands technology but they're not really a security specialist.

192
00:36:28.740 --> 00:36:32.070
Michael Shannon: The more that you can represent things graphically.

193
00:36:33.510 --> 00:36:43.710
Michael Shannon: The more meaningful that data is the better, especially when it comes to having to justify future outlays for more resources.

194
00:36:44.160 --> 00:37:00.540
Michael Shannon: For more controls or to hire more people or to send people to get more specialized training in let's say incident handling or forensics or writing Python scripts for vulnerability assessment.

195
00:37:03.960 --> 00:37:22.680
Michael Shannon: There are you know if you can have on your team good Python programmers maybe people that you know are know or learn the our language, which has some excellent ways to create better visualization you know.

196
00:37:24.810 --> 00:37:35.400
Michael Shannon: Avoiding just just you know here's pie charts let's come up with something a little bit more creative scatter plot density plots box blots things that can.

197
00:37:36.630 --> 00:37:41.700
Michael Shannon: represent that information in a more meaningful way, especially to.

198
00:37:43.020 --> 00:37:49.620
Michael Shannon: The recipients of that report that don't have the the knowledge is, you have the technical expertise.

199
00:37:50.940 --> 00:37:55.260
Michael Shannon: And you can't talk down to these decision makers.

200
00:37:56.940 --> 00:38:07.290
Michael Shannon: We have to be very careful as security practitioners that you know we don't, we have to maintain our patients, and you know, keep our cool.

201
00:38:08.550 --> 00:38:11.400
Michael Shannon: When we're dealing with people that don't have the you know.

202
00:38:12.480 --> 00:38:13.290
Michael Shannon: Understanding.

203
00:38:14.550 --> 00:38:21.420
Michael Shannon: You try to you try to explain to them, the importance of digitally signing something and they just don't get it.

204
00:38:24.570 --> 00:38:32.400
Michael Shannon: So meaningful metrics digestible results know your audience okay.

205
00:38:33.990 --> 00:38:42.420
Michael Shannon: These after action reports that you create whether it's after a test or after a real event after a vulnerability assessment.

206
00:38:43.230 --> 00:38:56.640
Michael Shannon: After a penetration test after a threat hunt maybe you've got red teams and blue team's you to send a bunch of people off to a conference for a week to learn how to be red team and blue team.

207
00:38:57.750 --> 00:39:06.840
Michael Shannon: And then you conduct some exercises some pen testing internally, you know all of those reports there's going to be lessons learned involved.

208
00:39:12.150 --> 00:39:13.350
Michael Shannon: Part of testing.

209
00:39:14.370 --> 00:39:25.200
Michael Shannon: and determining the maturity of your organization is to do threat modeling Now this is talking about threat modeling from the standpoint of evaluating malware.

210
00:39:26.070 --> 00:39:41.220
Michael Shannon: or evaluating the kill chain of some advanced persistent threat that you've gone two gone through so there's threat modeling you can do on the disposition of indicators of compromise that you found or the disposition of files that you found.

211
00:39:42.990 --> 00:39:52.650
Michael Shannon: Other artifacts threat modeling but there's also threat modeling from a software development testing standpoint as well.

212
00:39:53.250 --> 00:40:03.390
Michael Shannon: In this particular diagram and example you've got a sandbox environment, maybe a private cloud in your own enterprise it's running Type one hypervisor.

213
00:40:03.930 --> 00:40:16.980
Michael Shannon: that's just dedicated you know it's air gapped and it's just dedicated to evaluating malware and file disposition or you're sending it up to some cloud service to be evaluated.

214
00:40:18.060 --> 00:40:19.650
Michael Shannon: Cyber threat modeling.

215
00:40:21.180 --> 00:40:31.830
Michael Shannon: Now, in a cloud I would much rather if, as far as attacks that are happening to my cloud resources i'd much rather rely upon the cloud provider.

216
00:40:33.720 --> 00:40:59.100
Michael Shannon: That you know someone like aws guard duty, where you know I just turn it on and they're going to charge me for my usage of the cloud but guard duty uses, you know advanced machine learning and Ai it's a consortium of crowd strike trend rapid seven, you know that type of managed service.

217
00:41:00.750 --> 00:41:18.120
Michael Shannon: is worth the money, because for me to be able to emulate that in my own data Center on premise in my own threat modeling sandbox whatever i've set up my Type one or Type two hypervisor environment I there's no way I can come close to it.

218
00:41:22.410 --> 00:41:24.300
Michael Shannon: they've got the economy of scale.

219
00:41:26.070 --> 00:41:48.300
Michael Shannon: Now, as far as using threat modeling to find software vulnerabilities and weaknesses or threats during the process then that's going to involve some of the traditional models that have been around a long time, like stride strides been around what 99.

220
00:41:49.500 --> 00:41:52.200
Michael Shannon: It was you know us by.net programmers.

221
00:41:54.240 --> 00:42:12.540
Michael Shannon: stride focuses on kind of the categories of threats and that's what the acronym stands for spoofing tampering repudiation which repudiation is the opposite of non repudiation somebody is ability to deny that they took an action.

222
00:42:14.940 --> 00:42:18.300
Michael Shannon: I information disclosure data leakage.

223
00:42:19.530 --> 00:42:33.750
Michael Shannon: Data loss D denial of service E elevation or escalation of privileges, so you have different it's important to implement different threat modeling methodologies, they all have strengths and weaknesses.

224
00:42:35.250 --> 00:42:48.750
Michael Shannon: Vast is considered like the newest the new kid on the block vast supposedly is going to overcome weaknesses in some of these other methodologies.

225
00:42:50.040 --> 00:43:05.520
Michael Shannon: pasta dread stride trike okay vast is visual agile and simple threat modeling so it excels at it visualization it's agile it's much more flexible.

226
00:43:07.320 --> 00:43:09.960
Michael Shannon: And integrates with your devops portfolio.

227
00:43:11.340 --> 00:43:19.950
Michael Shannon: The goal is to you know address the limitations of other methodologies and you'll find you know security.

228
00:43:21.570 --> 00:43:29.400
Michael Shannon: Software as a service providers that maybe offer threat modeling services and they'll often be using vast.

229
00:43:32.580 --> 00:43:51.390
Michael Shannon: there's also understanding the risk of the supply chain and again this has really been one of those I mean there, there are literally organizations that are providing certifications for specialties not just with skater but, but you know supply chain risk.

230
00:43:52.470 --> 00:44:08.310
Michael Shannon: This has been you know disrupted and it's not just the disruptions of the chain, but you have vendors along the chain, who are operating at 70% capacity, and therefore they are not their budgets are not focused on privacy and security.

231
00:44:11.940 --> 00:44:31.680
Michael Shannon: Also, the supply chain disruptions affect us in our business continuity and our ability to replace or update from a from a physical or even a software device driver basis, our hardware and software security solutions, our security infrastructure.

232
00:44:34.080 --> 00:44:37.380
Michael Shannon: So there's a dedicated supply chain, risk management approach.

233
00:44:38.940 --> 00:44:43.290
Michael Shannon: Okay let's get into our practical cryptography discussion.

234
00:44:44.400 --> 00:44:54.750
Michael Shannon: Now, like I said, there is no dedicated cryptography domain, however, as we now start to move forward, you know we've we've accomplished our risk management.

235
00:44:56.160 --> 00:45:04.380
Michael Shannon: we're gonna start to look more specifically at the operational aspect of security as we move forward some more specifics.

236
00:45:10.290 --> 00:45:29.280
Michael Shannon: So let's make sure we have a kind of a nice even level playing field when it comes to our practical cryptography that's what we're concerned with Okay, we don't care what techniques as uses for confusion and diffusion.

237
00:45:30.690 --> 00:45:34.380
Michael Shannon: it's the way it shifts in the iterations no.

238
00:45:36.000 --> 00:46:01.230
Michael Shannon: No we're security managers were not cryptographers we're not crypt analysis specialist we're security managers so for centuries, since going back to recorded history, this is what we used symmetric key algorithms the same key was used that encrypted to decrypt and vice versa.

239
00:46:02.400 --> 00:46:04.320
Michael Shannon: Until we got to let's say I don't know.

240
00:46:05.700 --> 00:46:11.880
Michael Shannon: The Cold War, where we started to see asymmetric crypto systems emerge okay.

241
00:46:13.380 --> 00:46:14.040
Michael Shannon: So.

242
00:46:15.960 --> 00:46:26.190
Michael Shannon: The key has to be shared and that's a problem right that's a problem with these algorithms these systems because, how do you share that key in a secure way.

243
00:46:27.690 --> 00:46:32.550
Michael Shannon: Okay, and the tip it typically the key links are the other bit sighs.

244
00:46:34.110 --> 00:46:36.990
Michael Shannon: Okay into the whatever power.

245
00:46:38.370 --> 00:46:46.800
Michael Shannon: are smaller in length, because the math is simpler and we are doing, protection of bulk data.

246
00:46:48.450 --> 00:46:59.370
Michael Shannon: So typically and we will go eventually into higher key links in these algorithms were already on the way part of post quantum.

247
00:47:00.390 --> 00:47:01.290
Michael Shannon: cryptography.

248
00:47:03.600 --> 00:47:07.170
Michael Shannon: But symmetric key algorithms are still subject to brute force attack.

249
00:47:09.090 --> 00:47:18.090
Michael Shannon: Longer keys are less susceptible because it takes longer and more computing power to discover all the possibilities.

250
00:47:21.840 --> 00:47:25.860
Michael Shannon: To to the 512 power is a really large number.

251
00:47:27.030 --> 00:47:36.330
Michael Shannon: Still Okay, they provide wire speed encryption they're used for bulk data encryption we use them in our you know IP SEC vpn.

252
00:47:37.980 --> 00:47:41.850
Michael Shannon: We use symmetric keys to protect data at rest.

253
00:47:44.400 --> 00:47:57.420
Michael Shannon: with different cryptographic tools, whether it be bit locker or brother be some other third party tool full disk encryption whether we're doing you know encrypting objects at the cloud.

254
00:48:01.440 --> 00:48:10.260
Michael Shannon: You can have a stream cipher you can have a block cipher a block cipher operates on all the bits in the block simultaneously and then actually.

255
00:48:11.700 --> 00:48:35.160
Michael Shannon: Moving forward, they will chain those blocks together and encrypt and decrypt that chain of blocks to make it less deterministic or more secure stream ciphers typically do a exclusive or function, or some a rant pseudo random nonce of bits against a stream of zeros and ones.

256
00:48:39.690 --> 00:48:41.820
Michael Shannon: The original symmetric key.

257
00:48:42.840 --> 00:48:57.510
Michael Shannon: modes that we use in the digital world were based on what we did, for years, and hundreds or thousands of years or going back let's say to you know the world wars, where we had code books.

258
00:48:58.770 --> 00:49:04.200
Michael Shannon: were used so basically the original digitized.

259
00:49:05.400 --> 00:49:12.960
Michael Shannon: symmetric key crypto systems were just electronic versions of those code books which we rarely use anymore.

260
00:49:15.000 --> 00:49:22.080
Michael Shannon: Now CBC mode made the symmetric key algorithm much more.

261
00:49:23.400 --> 00:49:36.840
Michael Shannon: Stringent or stronger or more trustworthy by changing the blocks the fixed size blocks together 256 bits and equipping and encrypting the blocks as a chain.

262
00:49:39.090 --> 00:49:49.290
Michael Shannon: And then, to make them less deterministic those blocks as an as a algorithm as a whole are going to use what's called a nonce or a visualization vector.

263
00:49:50.460 --> 00:49:54.480
Michael Shannon: We started adding a counter to that to make it less deterministic.

264
00:49:55.620 --> 00:50:21.210
Michael Shannon: So you know cipher block chaining with counter mode is stronger less deterministic than just simply CBC or definitely let when it code book today the predominant symmetric key crypto system is a he is running in galois counter mode with galois fields at the heart of it.

265
00:50:23.610 --> 00:50:26.400
Michael Shannon: You ever have a some time this weekend.

266
00:50:28.800 --> 00:50:29.280
Michael Shannon: A.

267
00:50:32.460 --> 00:50:34.020
Michael Shannon: check out this dude.

268
00:50:35.970 --> 00:50:43.320
Michael Shannon: Every day galois he lived during the French Revolution, he was a mathematical genius but he was a troubled person is.

269
00:50:44.700 --> 00:50:48.360
Michael Shannon: He he kind of witnesses father died at 14.

270
00:50:49.710 --> 00:50:52.290
Michael Shannon: kind of a kind of a revolutionary.

271
00:50:53.940 --> 00:51:03.900
Michael Shannon: couldn't get into the best university, so he had to go to the second best university got an argument over a woman went to a duel.

272
00:51:04.800 --> 00:51:16.410
Michael Shannon: Or it was challenged to a duel and supposedly this is arguable but the night before he supposedly wrote down everything because he he assumed he wasn't going to survive the duel.

273
00:51:17.370 --> 00:51:37.620
Michael Shannon: So he's a 20 year old not an expert marksman he's a mathematician so he writes down everything every theorem every idea goes to the duel gets shot in the stomach dies the university comes in grabs up all of his papers and from there came galois fields.

274
00:51:38.700 --> 00:51:39.810
Michael Shannon: If he only would have known.

275
00:51:41.760 --> 00:51:46.590
Michael Shannon: galois counter mode can be run as an ad.

276
00:51:47.940 --> 00:51:49.320
Michael Shannon: will come back and talk about that.

277
00:51:53.700 --> 00:52:04.260
Michael Shannon: that's what it looks like that's how it works you don't need to really know that for the exam but it's in there, as your reference as the de facto standard, especially in United States.

278
00:52:05.310 --> 00:52:15.090
Michael Shannon: So that's symmetric key crypto systems asymmetric crypto system is really started to emerge in the Cold War let's say the MID to late 50s.

279
00:52:16.530 --> 00:52:37.200
Michael Shannon: Up until then everything was a symmetric algorithm with an asymmetric system, there is a mathematically related pair of keys a private key which needs to remain private and secret and then a public key which you can distribute to whoever right so.

280
00:52:38.310 --> 00:52:57.690
Michael Shannon: For privacy, we have allison Bob right so book allison Bob say Alice generates a key pair and i'll give you a quick little story on that in a second but Alice generates a key pair Okay, if she wants privacy okay she's going to get bob's.

281
00:52:58.710 --> 00:53:12.840
Michael Shannon: Public key for confidentiality she'll get bob's public key could be an out of band method, it can be through some handshake but she has definitely preferable some authenticated channel, but she gets bob's public key.

282
00:53:13.650 --> 00:53:23.130
Michael Shannon: She encrypts it with bob's public key and now Bob can use his private key to decrypt it that's for confidentiality or.

283
00:53:24.510 --> 00:53:32.010
Michael Shannon: Privacy if Alice wants wants Bob to have a high degree of confidence that she is the origin.

284
00:53:33.540 --> 00:53:37.620
Michael Shannon: She Bob will get her public key in some.

285
00:53:39.060 --> 00:53:53.790
Michael Shannon: channel she will encrypt it with her private key Bob will decrypt with her public key and he has a high degree of confidence that it's Alice because Alice had her private key now.

286
00:53:54.720 --> 00:54:08.220
Michael Shannon: The challenge, there is, how does Bob know that that was really Alice How does Bob know that alice's private key wasn't stolen or compromised.

287
00:54:09.870 --> 00:54:10.560
Michael Shannon: Well, he doesn't.

288
00:54:11.670 --> 00:54:20.850
Michael Shannon: So origin authentication obviously is nice but it's not enough what would be really helpful is if Alice and Bob.

289
00:54:22.410 --> 00:54:25.950
Michael Shannon: Both relied on a trusted third party.

290
00:54:26.970 --> 00:54:34.560
Michael Shannon: They would validate that that private key is still valid, that would be helpful.

291
00:54:36.000 --> 00:54:36.480
Michael Shannon: wouldn't it.

292
00:54:41.700 --> 00:54:51.960
Michael Shannon: Asymmetric keys if you encrypt with one you decrypt with the other, these are mathematically related think of them as fraternal twins okay.

293
00:54:53.850 --> 00:55:19.620
Michael Shannon: The key ranges are much higher so at you know we were getting to hierarchies let's say two to the 4096 power that is a number that is like virtually impossible to even imagine, but you use you use larger key spaces, with a symmetric keys because you're not protecting bulk data.

294
00:55:21.630 --> 00:55:38.760
Michael Shannon: you're basically protecting or encrypting the amount of information that you would find in the the metadata of your passport of your your national ID card the fields of your driver license or how about the fields.

295
00:55:39.780 --> 00:56:01.140
Michael Shannon: In a X 509 v3 certificate that's or maybe you're protecting a 256 or 512 bit session symmetric key Those are the kinds of things you protect with a symmetric key algorithms, but they can do other things as well.

296
00:56:02.250 --> 00:56:02.670
Michael Shannon: Okay.

297
00:56:04.320 --> 00:56:06.150
Michael Shannon: The most popular is RSA.

298
00:56:07.590 --> 00:56:12.360
Michael Shannon: But we still we're seeing you know elliptic curve dsa making a.

299
00:56:13.860 --> 00:56:19.860
Michael Shannon: You know, making making waves alright so here's a quick little story so.

300
00:56:21.870 --> 00:56:31.470
Michael Shannon: In the early days of the Internet, the arpanet where you did have like universities and other people getting on, and using this original Internet.

301
00:56:34.440 --> 00:56:47.520
Michael Shannon: The NSA the D O D military intelligence, they were using already a symmetric key crypto systems they have they've been they were developed in the late 50s they were using them.

302
00:56:48.510 --> 00:56:59.430
Michael Shannon: And a couple of guys, who are using the arpanet were like you know it would be really nice if we could have a system like that, so we can start sharing our public keys.

303
00:56:59.910 --> 00:57:20.220
Michael Shannon: On this arpanet thing with multiple hosts and entities universities partners research people whatever because symmetric keys they're difficult to manage and rotate and an exchange, you know.

304
00:57:21.540 --> 00:57:30.660
Michael Shannon: We really need something like they're using so Whitfield defeat and Martin hellman went to the NSA and said hey.

305
00:57:31.110 --> 00:57:44.460
Michael Shannon: We created this algorithm it's pretty cool and we can use it in the public and use it and we can use a commercially and it's totally different than what you're using so that's not going to be an issue the NSA said no.

306
00:57:45.120 --> 00:57:54.570
Michael Shannon: You don't get to have a asymmetric public key algorithm so defeat hellman went to the scientific American magazine.

307
00:57:55.440 --> 00:58:13.590
Michael Shannon: And they talked to the guy who wrote on the back of the magazine the puzzles you know, like the mensa stuff for the puzzle page that you know scientists would want to do for fun and they said hey would you publish our algorithm.

308
00:58:15.510 --> 00:58:18.870
Michael Shannon: are a symmetric key crypto system.

309
00:58:20.580 --> 00:58:23.130
Michael Shannon: And they the guys like yeah i'll do it, you know.

310
00:58:24.480 --> 00:58:31.320
Michael Shannon: So it was this right, so it was this with an explanation right of how.

311
00:58:32.550 --> 00:58:37.950
Michael Shannon: They could use this crypto system, so it got published.

312
00:58:38.970 --> 00:58:50.040
Michael Shannon: And the NSA well they couldn't they couldn't do anything, because it was it was the magazine was read all over the world, so what the NSA did to get in front of the story.

313
00:58:50.520 --> 00:59:04.980
Michael Shannon: was like we're gonna have a contest that's it let's have a contest to come up with the chosen, you know, a symmetric public key crypto system that can be used well, of course, defeat hellman they didn't win.

314
00:59:06.090 --> 00:59:19.860
Michael Shannon: Reverse Shamir and adelman won the contest and because of that, the those three men revenge Shamir adelman they became the de facto a symmetric key algorithm, which is still use today.

315
00:59:21.420 --> 00:59:40.500
Michael Shannon: In different forms the de facto standard, however defeat hellman realized, you know we lost them we lost the battle, but we didn't lose the war, because we can still use our a symmetric key algorithm to do key exchange, so we can generate.

316
00:59:42.120 --> 01:00:03.450
Michael Shannon: Key shared keys session keys between two parties over an untrusted network, so we don't have to like use out of band methods or whatever to get these shared keys exchange our protocol will do it, and they will prevent a person in the middle.

317
01:00:04.680 --> 01:00:19.080
Michael Shannon: to determine that now there's plenty of ways, where you can go and see how this works, the Wikipedia page shows you colors first how a how these a shared color is derived.

318
01:00:20.430 --> 01:00:31.410
Michael Shannon: But you can also if if this concept is still kind of new to you, you can go look up a art of the problem.

319
01:00:35.520 --> 01:00:36.720
Michael Shannon: art of the problem.

320
01:00:38.040 --> 01:00:39.270
Michael Shannon: By brick cruise.

321
01:00:40.800 --> 01:00:42.780
Michael Shannon: You should be able to find it on YouTube.

322
01:00:46.380 --> 01:00:59.820
Michael Shannon: I think Khan academy has his videos there too, and he does a really good job with them in like three minutes explaining a bunch of cryptographic principles and he walked through the dp hellman exchange.

323
01:01:00.360 --> 01:01:09.960
Michael Shannon: So there's your backstory on that dippy hellman has different groups we call these modulus it's just the key spaces, the key size.

324
01:01:11.100 --> 01:01:22.500
Michael Shannon: Early on, when we use these for example with IP SEC, because you want to generate you know shared secret keys between two parties in a site to site vpn.

325
01:01:23.340 --> 01:01:42.510
Michael Shannon: You know, we didn't have really big modulus but today we want to you know start with group 14 20,048 bit modulus okay so modulus it goes back to the modulo math.

326
01:01:43.620 --> 01:02:01.170
Michael Shannon: With that Britt crews will explain, but the original dippy hellman key exchange used, you know clock map where your where your finite space is the 12 o'clock modulo so the original case ends uses module that's why it's called modulus.

327
01:02:02.370 --> 01:02:16.980
Michael Shannon: When you get into more recent iterations of divvy hellman they use elliptic curves Okay, and the advantage of a list of elliptic curve, is that with much smaller key spaces.

328
01:02:17.550 --> 01:02:30.660
Michael Shannon: You can get better security than with a large modulus so the kind of the evolution has gone from the dippy hellman key exchange which some you'll see some vpn still using that.

329
01:02:32.010 --> 01:02:34.530
Michael Shannon: A femoral means the key is.

330
01:02:36.090 --> 01:02:41.010
Michael Shannon: For each key establishment process it's an ephemeral key that evaporates.

331
01:02:42.960 --> 01:02:48.450
Michael Shannon: And then, if it's elliptic curve it's using elliptic curve math not the modulo type math.

332
01:02:49.530 --> 01:03:00.720
Michael Shannon: And elliptic curve is considered not just more secure, but it's better because of all the different mobile devices and iot because it doesn't have the same type of overhead.

333
01:03:01.590 --> 01:03:13.980
Michael Shannon: And because mobile devices other smaller iot and other things like sensors and especially devices don't have all the processing power and the Ram elliptic curve is much more efficient.

334
01:03:15.750 --> 01:03:28.830
Michael Shannon: Now I haven't even mentioned all these things we're talking about so far are delivering confidentiality, but the mechanism that we use as an integrity mechanism is cryptographic hashing.

335
01:03:30.150 --> 01:03:32.010
Michael Shannon: So let's say a.

336
01:03:33.690 --> 01:03:40.020
Michael Shannon: Helen that's a good question I would say, on the exam be aware of use case.

337
01:03:41.400 --> 01:03:59.460
Michael Shannon: be aware of you know, my group 14 being the minimum acceptable, the fact that it uses dippy hellman key exchange the original modulus math but that we prefer to use elliptic curve because notice that group.

338
01:04:00.810 --> 01:04:02.910
Michael Shannon: is only a 394 bit.

339
01:04:04.140 --> 01:04:04.590
Michael Shannon: Right.

340
01:04:05.970 --> 01:04:12.690
Michael Shannon: Whereas group 14 is a 2048 bit, this is actually more secure.

341
01:04:14.910 --> 01:04:19.080
Michael Shannon: more trustworthy, we could say than the 2048 bit.

342
01:04:20.220 --> 01:04:23.700
Michael Shannon: modulus of a traditional elliptic curve key exchange.

343
01:04:24.900 --> 01:04:26.550
Michael Shannon: that's some of the takeaways.

344
01:04:28.050 --> 01:04:42.270
Michael Shannon: And the fact that we're generating shared secret keys over a untrusted network here's something else Helen that you might want to think about if this is a concept that you haven't been familiar with.

345
01:04:47.670 --> 01:04:49.140
Michael Shannon: let's say you're using.

346
01:04:51.180 --> 01:05:12.990
Michael Shannon: dippy hellman to generate shared secret keys that you're going to use in your IP SEC vpn okay part of the security profile that you choose that has to match the other site is what dippy hellman motor you using core group so you have to have a matching group, like say group 14.

347
01:05:14.370 --> 01:05:21.960
Michael Shannon: let's say that you both decide we're going to use group 14 we're going to use elliptic curve dippy hellman group 14 okay.

348
01:05:23.580 --> 01:05:34.260
Michael Shannon: you're going to generate a shared secret key, but also that key can be used to generate additional King material so, for example, you have these two sites.

349
01:05:35.040 --> 01:05:43.440
Michael Shannon: let's say there's two Cisco routers and we set up a site to site vpn and our initial peering will be for a day.

350
01:05:44.070 --> 01:05:57.090
Michael Shannon: 40,000 some whatever sits in seconds so 24 hours are appearing security association is set up 24 hours now within 24 hours we have.

351
01:05:57.630 --> 01:06:14.760
Michael Shannon: A few or a bunch or many IP SEC sessions, and they have much shorter lifetimes like five minutes or 30 minutes you follow me in in the IP SEC sessions, there you know, protecting the data with whatever they use.

352
01:06:17.520 --> 01:06:19.530
Michael Shannon: With perfect forward secrecy.

353
01:06:20.730 --> 01:06:32.940
Michael Shannon: If you don't have her before it secrecy, you would do the initial dippy hellman exchange and for 24 hours you would generate keys from that key.

354
01:06:34.110 --> 01:06:44.430
Michael Shannon: That key would generate keys what perfect forward secrecy does and it's not a sign apply the same way everywhere, but the way we work with IP SEC.

355
01:06:45.270 --> 01:07:04.860
Michael Shannon: Is that every time you did a new IP SEC session let's say every hour, you would do a new defeat hellman exchange so multiple dippy hellman exchanged for every IP SEC session within the 24 hour period perfect forward secrecy.

356
01:07:07.470 --> 01:07:21.120
Michael Shannon: That may be something that you would need to be aware of, on the exam that that's recommended that your crypto systems use forward secrecy or perfect forward secrecy, so that some key.

357
01:07:21.450 --> 01:07:29.970
Michael Shannon: That gets compromised in the future can't be used to go back and attack other older previously generated keys.

358
01:07:31.740 --> 01:07:34.500
Michael Shannon: That might be something that you might need to be aware of.

359
01:07:35.610 --> 01:07:46.590
Michael Shannon: And the fact that we use elliptic curve dippy hellman because of mobile devices and iot they don't have that processing power or Ram memory.

360
01:07:49.110 --> 01:07:53.700
Michael Shannon: So a cryptographic hash probably the most common data of arbitrary length.

361
01:07:55.020 --> 01:07:57.330
Michael Shannon: Moving forward to be an API call.

362
01:07:58.380 --> 01:08:09.780
Michael Shannon: An application programming interface call or request let's just use that this is going to be a very common thing that we cryptographic we hash, whether in the cloud or in our own.

363
01:08:10.290 --> 01:08:22.950
Michael Shannon: You know, possibly our own data Center using controllers OK, so the data of arbitrary link goes through a hash function.

364
01:08:24.090 --> 01:08:41.130
Michael Shannon: Now one thing on the exam we don't use md five anymore Okay, because a cryptographic hash function has to be collision resistant, in other words, two different inputs into the hash function cannot create the same output, that would be bad.

365
01:08:42.300 --> 01:08:45.630
Michael Shannon: Right, because what if you had program emc.

366
01:08:46.980 --> 01:09:10.470
Michael Shannon: And then you had another thing called program xc, this is the actual program, but this will Nan program that emc is a Trojan horse malware if both of those with the name program xc went through the hash function and generated the same fixing pash that's not a good thing.

367
01:09:12.210 --> 01:09:14.430
Michael Shannon: And that's been accomplished with empty file.

368
01:09:15.600 --> 01:09:22.080
Michael Shannon: So we don't use empty file and really we want to use sha two or the shot three family.

369
01:09:23.550 --> 01:09:34.050
Michael Shannon: Okay, so it produces They fix the password digest or the fingerprint and it's theoretically and an irreversible mathematical function.

370
01:09:35.100 --> 01:09:39.300
Michael Shannon: Now, can you guess the input yeah.

371
01:09:40.380 --> 01:09:43.350
Michael Shannon: yeah you can guess some or part of the input.

372
01:09:45.390 --> 01:09:46.380
Michael Shannon: Now, would be helpful.

373
01:09:47.640 --> 01:10:02.730
Michael Shannon: But it's irreversible mathematically it's like taking a bunch of coffee beans, putting them in a coffee grinder and having your coffee and trying to reverse that process now a couple of things about cryptographic hashing.

374
01:10:04.590 --> 01:10:10.110
Michael Shannon: Obviously, they need to be collision resistant, I mentioned that in the last bullet point also.

375
01:10:11.880 --> 01:10:12.600
Michael Shannon: They.

376
01:10:14.460 --> 01:10:30.300
Michael Shannon: Have what are called the avalanche effect so let's say your API call Okay, if you put the API call to the hash and it gives you a fixed thing past let's say you went to the API call like the code and you just changed one character.

377
01:10:31.590 --> 01:10:47.040
Michael Shannon: Okay, in the original API and you send it through the hash the avalanche effect it would be totally different it wouldn't like change one character of the fingerprint of the 256 bits it would be completely different like an avalanche.

378
01:10:48.120 --> 01:10:54.390
Michael Shannon: The other factor is the birthday paradox which revolves around the number 23.

379
01:10:56.910 --> 01:10:58.290
Michael Shannon: Okay, like the job, like the.

380
01:10:59.400 --> 01:11:04.230
Michael Shannon: Jim carrey movie 23 is a special number so.

381
01:11:05.400 --> 01:11:08.490
Michael Shannon: According to the birthday paradox.

382
01:11:10.020 --> 01:11:19.890
Michael Shannon: In a group this size okay there's a fit there's a 5050 chance that somebody else has the same birthday as me.

383
01:11:21.390 --> 01:11:34.530
Michael Shannon: not born on the same day, the same calendar birthday 5050 chance the birthday paradox, and when you apply the birthday paradox to a cryptographic hash.

384
01:11:36.210 --> 01:11:53.040
Michael Shannon: What it means is shot to 256 bit is actually only 128 bits it's only half of the advertised string now that only applies to hashing algorithms.

385
01:11:54.420 --> 01:12:03.600
Michael Shannon: Okay, but empty file has been dedicated and even a lot of times we avoid sha one now for for these okay.

386
01:12:04.740 --> 01:12:21.780
Michael Shannon: cryptographic hashing now cryptographic hashing is really just to check some on steroids, we use check sums all the time when you copy a file from you know one drive to another or your frames are sent between devices there's a check some.

387
01:12:22.920 --> 01:12:39.480
Michael Shannon: A frame check sequence or CRC whatever to make sure the frame wasn't corrupted in transit or when you copy the file over that it was exactly the same as the source file, so a hashing our algorithms like a check some on steroids.

388
01:12:41.130 --> 01:12:57.030
Michael Shannon: But that's just an integrity mechanism, we have to add something to that to get integrity and origin authentication so let's say we've got two routers and it's very common for routers or routers to send.

389
01:12:58.260 --> 01:13:13.770
Michael Shannon: Information to each other advertisements Hello packets updates, but with regards to the protocol, they are communicating with each other, because they are peers, they are neighbors in some type of domain routing domain.

390
01:13:14.550 --> 01:13:21.960
Michael Shannon: Okay, whatever the protocol rip version to ospf ei gop border gateway protocol don't know don't care.

391
01:13:23.610 --> 01:13:34.110
Michael Shannon: They want to make sure that these packets they're getting from their supposed neighbor have not been modified in transit and is actually coming from the origin, I expected to be.

392
01:13:34.680 --> 01:13:47.940
Michael Shannon: Because you can mess with you mess up somebody's routing table you mess up their entire network infrastructure, so the route, the the routers are going to configure a shared key.

393
01:13:48.840 --> 01:14:08.310
Michael Shannon: Both of them have to have the same shared key Okay, the routing update, which is the input, you know the the input, the data of arbitrary length right and this example happens to be a routing update the router a wants to tell router be I don't know why these are both called.

394
01:14:09.600 --> 01:14:13.110
Michael Shannon: router a router at a fixie say how did that get through.

395
01:14:14.460 --> 01:14:25.710
Michael Shannon: So what's going to happen is that shared key that they both have the key gets interleaved with the update and that goes through the hash function.

396
01:14:26.490 --> 01:14:40.080
Michael Shannon: And that's called an H MAC so often what we do is be used the H MAC as part of a security suite whether it's IP SEC or ssl or whatever there's a security suite and we use an H MAC.

397
01:14:40.740 --> 01:14:52.770
Michael Shannon: sha one shot to shot three for an integrity in origin authentication and a separate symmetric key system like three days or eight yes for confidentiality.

398
01:14:54.240 --> 01:15:05.610
Michael Shannon: Okay, and by the way, when you're you have a vpn you have to agree on what you're going to use okay let's go back to it, yes, because this may come up on the exam.

399
01:15:10.110 --> 01:15:12.000
Michael Shannon: As is.

400
01:15:18.150 --> 01:15:20.970
Michael Shannon: referred to as an ad.

401
01:15:22.410 --> 01:15:29.400
Michael Shannon: authenticated and critter authenticated descriptor so if you're using the.

402
01:15:30.840 --> 01:15:32.430
Michael Shannon: where's my galois counter mode.

403
01:15:33.990 --> 01:15:34.530
Michael Shannon: slide.

404
01:15:37.080 --> 01:15:38.910
Michael Shannon: If you're using as.

405
01:15:44.220 --> 01:15:48.870
Michael Shannon: And this is what's commonly used at service providers cloud providers.

406
01:15:49.950 --> 01:15:54.690
Michael Shannon: it's an ad it has its own G MAC.

407
01:15:55.740 --> 01:16:09.180
Michael Shannon: As part of the algorithm so you don't have to have a separate shot to or shot three H MAC when you're using as gcm 256 with the gmac.

408
01:16:10.860 --> 01:16:21.030
Michael Shannon: it's integrated it's an authenticated and character authenticated descriptor that's one of the advantages of using a a yes.

409
01:16:23.100 --> 01:16:25.680
Michael Shannon: You don't need to separate H MAC OK.

410
01:16:27.780 --> 01:16:28.200
Michael Shannon: Now.

411
01:16:29.610 --> 01:16:35.340
Michael Shannon: let's uh let's take our final break and we'll come back and we'll finish up encryption.

412
01:16:37.050 --> 01:16:45.390
Michael Shannon: And then, our final topic is identity and access management and i'm not gonna blow through it i'm not going to rush through it.

413
01:16:46.530 --> 01:16:57.600
Michael Shannon: In this course, I have time built in today five other words my day five slide deck is shorter.

414
01:16:58.500 --> 01:17:10.500
Michael Shannon: Because i'm not going to rush through any of this will come back from break we'll finish up the cryptography stuff talk about pk I and some other things, you need to know about and we'll get into identity.

415
01:17:10.950 --> 01:17:17.940
Michael Shannon: And access management and then wherever we're done at the at the top of the next hour will take up there tomorrow.

416
01:17:18.660 --> 01:17:30.600
Michael Shannon: i'm not rushing through anything i'm not blowing through anything I might give you a reading assignment now I may give you a reading assignment on some things that we in the next couple of days that are repetitive.

417
01:17:31.830 --> 01:17:39.180
Michael Shannon: But not not on these topics way too important for the exam so i'm going to pause the recording or stop the recording.

418
01:17:41.250 --> 01:17:42.990
Michael Shannon: i'll see you back here in 15 minutes.

